---
title: 机器学习 | 逻辑回归核心理论篇
date: 2024-06-16
tags:
  - Python
  - MachineLearning
  - Statistics
  - LogisticRegression
draft: false
summary: 此处写描述
authors:
  - default
images:
  - /static/images/cat.gif
  - /static/images/tech/Logistic/Figure_group1.png
  - /static/images/tech/Logistic/Figure_group2.png
  - /static/images/tech/Logistic/sigmoid.png
---

[ 最后更新 于  2024/6/16]

引言：
发现之前对逻辑回归是非常一知半解的，尤其是sigmoid函数的正确形式和推导方面。这可能就是调包侠的大失败吧（滑稽脸）。所以今天就开始从零到一仔细复盘一下逻辑回归。

## 有了线性回归 为什么我们还需要逻辑回归？

线性回归常常用于预测连续值，但是~~很多人~~（我自己）往往会想，为什么就不可以用来0/1分类呢？一个很自然的想法就是超过0.5的归为1，不到0.5的归为0就可以了。然而，这个看似合理的想法存在不合理的地方。其重要的一点就在于，这种从0到1的变化往往并不是线性的，而是在某个临界点发生了一个质变，而线性函数往往不具备较准确的预测这个临界点的能力。


我们以根据肿瘤大小来预测其风险为例：

首先，我们看到线性回归给出的y值可能是小于0的，也可以是大于1的，倘若将这个值解读为概率，似乎显然是不合适的。不过可能有人说，“那我们就忽略这些，只以0.5为界来分类而已，似乎这两种回归差别也不大嘛。”

![Figure_group1](/static/images/tech/Logistic/Figure_group1.png) 

那么再考虑以下这种情况：假如数据中包含一个很大的肿瘤，显然是恶性的，那么如果是线性回归的情况下，这一极端值会影响让整个回归线的趋势，使得整体数据对于恶性肿瘤的风险估计不足，甚至误判（见红圈内的数据）。这使得我们不得不考虑另一种回归曲线了。

![Figure_group2](/static/images/tech/Logistic/Figure_group2.png) 


## Sigmoid函数与Odd

经过上面对于线性函数短板的讨论，我们已经得出了x与y之间关系这种非线性的特征：即在某临界值前后变化明显，而后保持不变的特征。这不得不让人想到描述生物种群大小、人口增长中经常涉及到的S型曲线。这也正是逻辑回归时普遍所使用的s型函数。其解析式和图像如下：

$$\Phi(z) = \frac {1}{1 + e^{-z}}$$

![sigmoid.png](/static/images/tech/Logistic/sigmoid.png)

第一次看到这个式子，或许很多人都和我一样头大，一眼不能看清这个式子该怎么理解。这里就不得不先介绍一下 **odd** 这个概念。

### 什么是Odd？

笔者并不知道odd这个词的官方中文翻译是什么，查了各大百科和一些笔记，对odd这个词的翻译包括但不限于 发生比、优势比、赔率、几率…
不过要说清楚odd究竟是什么，看公式是最快的：

$$Odd=\frac{P(x)}{1-P(x)}$$

这个公式中P(X)就是我们所熟悉的一般意义下的概率了。比如，如果P(X)是0.8，也就是发生的概率是80%，那么Odd就是4，如果P(X)是0.9，那么Odd就变为9了，假如P(X)是0.95，Odd增加到了19。反过来，若P(X)=0.2, Odd变为0.25。由此观之，Odd的增加和减小可以说是指数倍的，难怪这一概念据说最早源自赌马比赛，和目前体育彩票中赔率的概念可以说是相通的（尽管赔率越高代表赢面越小欸）。

讲到这里，我们不难想到刚刚肿瘤大小的例子，线性回归的结果不仅有正有负，还不在零一之间，倘若把它看作概率是说不过去的，但是若把它看作是取了对数后的odd，便好解释很多了。

也就是说我们在此假设:

$$Log(Odd)=wx+b$$（此处Log表达以e为底的对数函数，同Ln）

$$Log(\frac {P(x)}{1-P(x)})=wx+b$$

有：

$$\frac {P(x)}{1-P(x)}=e^{wx+b}$$

$$P(X)=e^{ex+b}-P(x)e^{wx+b}$$

$$P(X)(1+e^{wx+b})=e^{wx+b}$$

得：

$$P(X)=\frac{e^{wx+b}}{1+e^{wx+b}}$$

稍作化简即得到之前常用的s型曲线的形式：

$$P(X)=\frac {1}{1+e^{-z}}$$

$$z=wx+b$$

注意，此处的z并不非得是wx+b，它当然也可以是一个二次曲线或别的什么，只要你觉得可以用该$e^z$来拟合odd值。

此时我们就得到了logistic function的一个标准形式，即：

	$$y=\frac {1}{1+e^{-z}}$$

而y总在0到1之间，表示概率。一般根据0.5为界划分类别，但也可以根据实际情况进行调整。例如，为了规避风险，把有30%以上概率成为诈骗邮件的邮件都归为诈骗邮件。

## Cost Function和梯度递降

确定了回归要使用的函数，按照之前线性回归中我们进行过的步骤，我们还是想要用梯度递降法来寻找最佳的拟合曲线，就不得不先确定cost function从而进一步进行梯度递降。

那么 逻辑回归的损失函数是什么呢？

### 最大似然法（maximum likelihood estimation）

（我真的很讨厌这个故弄玄虚的中文名。以下就用maximum likelihood来代替）

在线性回归中，每个点的损失我们使用非常直观的距离平方来确定，而在逻辑回归中，我们则希望尽可能地把数据归入正确的类别之中。于是从概率而非距离的角度来看这个问题：
我们想寻找到最佳曲线——使得每个点**都**被归入正确类别的概率最大：
也就是我们想要最大化下面这个函数：
$$Likelihood=\Pi_{{x_i}|y_i=0}(1-\hat{y}_{x_i})\Pi_{x_j|y_j=1}\hat{y}_{x_j}$$
我自己的理解是，该函数可以看作一个预测整体的正确率。比方说，若某点预测出为阳性（1）的概率为20%，而其真实的值又为0，那么该预测的概率就是80%，反之，如果一个点预测出结果为阳性为20%，而其真实的值其实1，那么该预测的正确率就是20%。将所有点的正确率相乘，就得到了该函数基于全部样本预测的整体正确率。而我们需要找到的曲线就是能够最大化这个值的曲线。

桥豆麻袋，还没结束。这个函数显然是需要最大化的函数，而我们找的对应的lost function还需要一点点额外的处理。
1）先对上面这个求积公式取对数，这或许是因为让计算机做乘法稍微有点痛苦以及没有必要：注意，这个地方用两个系数来区分yi等于1或0的情况（总有一个为零）。
$$log(likelihood)=\Sigma[{(1-y_i)log(1-\hat{y}_{x_i})}+(y_i)log(\hat{y}_{x_i})]$$
2）再对这个值取反后求平均值，就得到了我们熟悉的cost function的形式：
$$J=-\frac{1}{m}\Sigma[{(1-y_i)log(1-\hat{y}_{x_i})}+y_ilog(\hat{y}_{x_i})]$$

### 梯度递降

说完了cost function，显然进入了熟悉的通过偏导求梯度的时刻：

尽管结论十分简洁甚至形式和线性回归一样:
$\frac{\partial J}{\partial \mathbf{w}} = \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{(i)} - y^{(i)} \right) x^{(i)}$
$\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^{m} \left( \hat{y}^{(i)} - y^{(i)} \right)$

但要想到达这个直观的结论，似乎还是要一些计算，我们以对w偏导为例：

先设$g(z)=\frac{1}{1+e^{-z}}$ , $z(w)=wx+b$，

$$\frac{\partial g}{\partial w} =\frac{\partial g}{\partial z} \frac{\partial z}{\partial w}=\frac{e^{-z}}{(1+e^{-z})^2}x$$

(注意到：$\hat{y}^{(i)}=\frac {1}{1+e^{-z}}$)

$$=(1-\hat{y}^{(i)})\hat{y}^{(i)}x$$

于是

$$\frac{\partial J}{\partial \mathbf{w}} =-\frac{1}{m}\Sigma[\frac{-(1-\hat{y}^{(i)})\hat{y}^{(i)}(1-y^{(i)})x}{1-\hat{y}^{(i)}}+\frac{(1-\hat{y}^{(i)})\hat{y}^{(i)}y^{(i)}x}{\hat{y}^{(i)}}]$$$$=-\frac{1}{m}\Sigma(-\hat{y}^{(i)}x+y^{(i)}x)=\frac{1}{m}\Sigma(\hat{y}^{(i)}-y^{(i)})x$$

注意，若有w为向量$（w_1,w_2,..w_j）$，则上式中x需要取与与该$w_j$对应的$x_j$

于是我们就得到了，梯度递降每次更新的核心公式。

至此，我们基本已经得到了完成逻辑回归代码的全部理论支撑。甚至，这个梯度骤降和linear regression惊人的相似（我还是想知道是否是巧合），所以这一部分的过程甚至是无需改动的。

下周我们再更新实际的操作吧~

---

![Cat](/static/images/cat.gif)


`今天这篇写的比以往慢好多呀。感觉自己对于整个概念的逻辑已经很努力地在顺了，可是对于使用S型函数还是有一些地方没有完全想明白。(*・_・)ノ⌒* 但是ChatGPT告诉我之所以选用S型curve是因为它的可解释性、可导性以及实际应用的偏好吧。(￣▽￣)ノ 我姑且尊重历史的选择以及统计学家的直觉。(^_^;)`

`这周把Coursera上Machine Learning Specialisation的第一部分关于Regression and Classification的部分大致看完了。(^_^)v 然而笔记还没有完全整理完。(>_<) 推导公式和复合求导以及一大长串的式子是我的一生之敌… (Ｔ▽Ｔ)`

`下周应该要开始看第二部分更深入的一些算法，顺便写关于实操和正则化的一些问题。( •̀ ω •́ )✧`


-鹅仔
2024/06/16 21:21
